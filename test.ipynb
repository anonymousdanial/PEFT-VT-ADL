{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anomaly Detection with Vision Transformers and Mixture Density Networks\n",
        "\n",
        "**Author:** Pankaj Mishra\n",
        "\n",
        "This notebook implements anomaly detection on the MVTech dataset using Vision Transformer Autoencoders (VT_AE) and Mixture Density Networks (MDN) for Gaussian approximation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import mvtech\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import pytorch_ssim\n",
        "from einops import rearrange\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import mdn1\n",
        "from VT_AE import VT_AE as ae\n",
        "from utility_fun import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "prdt = \"cable\"\n",
        "patch_size = 64\n",
        "\n",
        "# Initialize SSIM Loss\n",
        "ssim_loss = pytorch_ssim.SSIM() # SSIM Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset and Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset\n",
        "data = mvtech.Mvtec(1, product=prdt)\n",
        "\n",
        "# Model declaration\n",
        "model = ae(train=False).cuda()\n",
        "G_estimate = mdn1.MDN().cuda()\n",
        "\n",
        "# Loading weights\n",
        "model.load_state_dict(torch.load(f'./saved_model/VT_AE_Mvtech_{prdt}'+'.pt'))\n",
        "G_estimate.load_state_dict(torch.load(f'./saved_model/G_estimate_Mvtech_{prdt}'+'.pt'))\n",
        "\n",
        "# Put models in evaluation mode\n",
        "model.eval()\n",
        "G_estimate.eval()\n",
        "\n",
        "print(f\"Models loaded successfully for product: {prdt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing data loaders\n",
        "loader = [data.train_loader, data.test_norm_loader, data.test_anom_loader]\n",
        "\n",
        "# Initialize loss lists\n",
        "t_loss_norm = []\n",
        "t_loss_anom = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Thresholding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Thresholding(data_load=loader[1:], upsample=1, thres_type=0, fpr_thres=0.3):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_load : TYPE, optional\n",
        "        DESCRIPTION. The default is loader[1:].\n",
        "    upsample : INT, optional\n",
        "        DESCRIPTION. 0 - NearestUpsample2d; 1- BilinearUpsampling.\n",
        "    thres_type : INT, optional\n",
        "        DESCRIPTION. 0 - 30% of fpr reached; 1 - thresholding using best F1 score\n",
        "    fpr_thres : FLOAT, Optional\n",
        "        DESCRIPTION. False Positive Rate threshold value. Default is 0.3\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Threshold: Threshold value\n",
        "    '''\n",
        "    norm_loss_t = []\n",
        "    normalised_score_t = []\n",
        "    mask_score_t = []\n",
        "\n",
        "    for data in data_load:\n",
        "        for i, j in data:\n",
        "            if i.size(1) == 1:\n",
        "                i = torch.stack([i, i, i]).squeeze(2).permute(1, 0, 2, 3)\n",
        "            vector, reconstructions = model(i.cuda())\n",
        "            pi, mu, sigma = G_estimate(vector)\n",
        "            \n",
        "            # Loss calculations\n",
        "            loss1 = F.mse_loss(reconstructions, i.cuda(), reduction='mean')  # Rec Loss\n",
        "            loss2 = -ssim_loss(i.cuda(), reconstructions)  # SSIM loss for structural similarity\n",
        "            loss3 = mdn1.mdn_loss_function(vector, mu, sigma, pi, test=True)  # MDN loss for gaussian approximation\n",
        "            loss = loss1 + loss2 + loss3.sum()  # Total loss\n",
        "            norm_loss_t.append(loss3.detach().cpu().numpy())\n",
        "                \n",
        "            if upsample == 0:\n",
        "                # Mask patch\n",
        "                mask_patch = rearrange(j.squeeze(0).squeeze(0), '(h p1) (w p2) -> (h w) p1 p2', p1=patch_size, p2=patch_size)\n",
        "                mask_patch_score = Binarization(mask_patch.sum(1).sum(1), 0.)\n",
        "                mask_score_t.append(mask_patch_score)  # Storing all masks\n",
        "                norm_score = norm_loss_t[-1]\n",
        "                normalised_score_t.append(norm_score)  # Storing all patch scores\n",
        "            elif upsample == 1:\n",
        "                mask_score_t.append(j.squeeze(0).squeeze(0).cpu().numpy())  # Storing all masks\n",
        "                m = torch.nn.UpsamplingBilinear2d((512, 512))\n",
        "                norm_score = norm_loss_t[-1].reshape(-1, 1, 512//patch_size, 512//patch_size)\n",
        "                score_map = m(torch.tensor(norm_score))\n",
        "                score_map = Filter(score_map, type=0)  # add normalization here for the testing\n",
        "                normalised_score_t.append(score_map)  # Storing all score maps               \n",
        "                \n",
        "    scores = np.asarray(normalised_score_t).flatten()\n",
        "    masks = np.asarray(mask_score_t).flatten()\n",
        "    \n",
        "    if thres_type == 0:\n",
        "        fpr, tpr, _ = roc_curve(masks, scores)\n",
        "        fp3 = np.where(fpr <= fpr_thres)\n",
        "        threshold = _[fp3[-1][-1]]\n",
        "    elif thres_type == 1:\n",
        "        precision, recall, thresholds = precision_recall_curve(masks, scores)\n",
        "        a = 2 * precision * recall\n",
        "        b = precision + recall\n",
        "        f1 = np.divide(a, b, out=np.zeros_like(a), where=b != 0)\n",
        "        threshold = thresholds[np.argmax(f1)] \n",
        "    return threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Patch Overlap Score Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Patch_Overlap_Score(threshold, data_load=loader[1:], upsample=1):\n",
        "    \n",
        "    norm_loss_t = []\n",
        "    normalised_score_t = []\n",
        "    mask_score_t = []\n",
        "    loss1_tn = []\n",
        "    loss2_tn = []\n",
        "    loss3_tn = []\n",
        "    loss1_ta = []\n",
        "    loss2_ta = []\n",
        "    loss3_ta = []\n",
        "    \n",
        "    score_tn = []\n",
        "    score_ta = []\n",
        "    \n",
        "    for n, data in enumerate(data_load):\n",
        "        total_loss_all = []\n",
        "        for c, (i, j) in enumerate(data):\n",
        "            if i.size(1) == 1:\n",
        "                i = torch.stack([i, i, i]).squeeze(2).permute(1, 0, 2, 3)\n",
        "            vector, reconstructions = model(i.cuda())\n",
        "            pi, mu, sigma = G_estimate(vector)\n",
        "           \n",
        "            # Loss calculations\n",
        "            loss1 = F.mse_loss(reconstructions, i.cuda(), reduction='mean')  # Rec Loss\n",
        "            loss2 = -ssim_loss(i.cuda(), reconstructions)  # SSIM loss for structural similarity\n",
        "            loss3 = mdn1.mdn_loss_function(vector, mu, sigma, pi, test=True)  # MDN loss for gaussian approximation\n",
        "            loss = loss1 - loss2 + loss3.max()  # Total loss\n",
        "            norm_loss_t.append(loss3.detach().cpu().numpy())\n",
        "            total_loss_all.append(loss.detach().cpu().numpy())\n",
        "            \n",
        "            if n == 0:\n",
        "                loss1_tn.append(loss1.detach().cpu().numpy())\n",
        "                loss2_tn.append(loss2.detach().cpu().numpy())\n",
        "                loss3_tn.append(loss3.sum().detach().cpu().numpy())\n",
        "            if n == 1:\n",
        "                loss1_ta.append(loss1.detach().cpu().numpy())\n",
        "                loss2_ta.append(loss2.detach().cpu().numpy())\n",
        "                loss3_ta.append(loss3.sum().detach().cpu().numpy())\n",
        "                \n",
        "            if upsample == 0:\n",
        "                # Mask patch\n",
        "                mask_patch = rearrange(j.squeeze(0).squeeze(0), '(h p1) (w p2) -> (h w) p1 p2', p1=patch_size, p2=patch_size)\n",
        "                mask_patch_score = Binarization(mask_patch.sum(1).sum(1), 0.)\n",
        "                mask_score_t.append(mask_patch_score)  # Storing all masks\n",
        "                norm_score = Binarization(norm_loss_t[-1], threshold)\n",
        "                m = torch.nn.UpsamplingNearest2d((512, 512))\n",
        "                score_map = m(torch.tensor(norm_score.reshape(-1, 1, 512//patch_size, 512//patch_size)))\n",
        "                normalised_score_t.append(norm_score)  # Storing all patch scores\n",
        "            elif upsample == 1:\n",
        "                mask_score_t.append(j.squeeze(0).squeeze(0).cpu().numpy())  # Storing all masks\n",
        "                \n",
        "                m = torch.nn.UpsamplingBilinear2d((512, 512))\n",
        "                norm_score = norm_loss_t[-1].reshape(-1, 1, 512//patch_size, 512//patch_size)\n",
        "                score_map = m(torch.tensor(norm_score))\n",
        "                score_map = Filter(score_map, type=0)\n",
        "                normalised_score_t.append(score_map)  # Storing all score maps\n",
        "                \n",
        "            # Plotting\n",
        "            if c % 5 == 0:\n",
        "                plot(i, j, score_map[0][0])\n",
        "            if n == 0:\n",
        "                score_tn.append(score_map.max())\n",
        "            if n == 1:\n",
        "                score_ta.append(score_map.max())\n",
        "                \n",
        "        if n == 0:\n",
        "            t_loss_all_normal = total_loss_all\n",
        "        if n == 1:\n",
        "            t_loss_all_anomaly = total_loss_all\n",
        "        \n",
        "    # PRO Score            \n",
        "    scores = np.asarray(normalised_score_t).flatten()\n",
        "    masks = np.asarray(mask_score_t).flatten()\n",
        "    PRO_score = roc_auc_score(masks, scores)\n",
        "    \n",
        "    # Image Anomaly Classification Score (AUC)\n",
        "    roc_data = np.concatenate((t_loss_all_normal, t_loss_all_anomaly))\n",
        "    roc_targets = np.concatenate((np.zeros(len(t_loss_all_normal)), np.ones(len(t_loss_all_anomaly))))\n",
        "    AUC_Score_total = roc_auc_score(roc_targets, roc_data)\n",
        "    \n",
        "    # AUC Precision Recall Curve\n",
        "    precision, recall, thres = precision_recall_curve(roc_targets, roc_data)\n",
        "    AUC_PR = auc(recall, precision)\n",
        "\n",
        "    return PRO_score, AUC_Score_total, AUC_PR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Execution: Calculate Threshold and Evaluate Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate threshold\n",
        "print(\"Calculating threshold...\")\n",
        "thres = Thresholding()\n",
        "print(f\"Threshold calculated: {thres}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate performance metrics\n",
        "print(\"Calculating performance metrics...\")\n",
        "PRO, AUC, AUC_PR = Patch_Overlap_Score(threshold=thres)\n",
        "\n",
        "# Display results\n",
        "print(f'\\nResults for product: {prdt}')\n",
        "print(f'PRO Score: {PRO:.4f}')\n",
        "print(f'AUC Total: {AUC:.4f}')\n",
        "print(f'PR_AUC Total: {AUC_PR:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implements an anomaly detection system using:\n",
        "\n",
        "1. **Vision Transformer Autoencoder (VT_AE)**: For feature extraction and reconstruction\n",
        "2. **Mixture Density Network (MDN)**: For Gaussian approximation of the feature space\n",
        "3. **Multi-component Loss Function**:\n",
        "   - MSE reconstruction loss\n",
        "   - SSIM loss for structural similarity\n",
        "   - MDN loss for distribution modeling\n",
        "\n",
        "The evaluation includes:\n",
        "- **PRO Score**: Patch-level anomaly detection performance\n",
        "- **AUC Score**: Image-level classification performance\n",
        "- **PR-AUC**: Precision-Recall Area Under Curve\n",
        "\n",
        "The system supports both patch-level and pixel-level anomaly localization with configurable upsampling methods."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}