{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661718bb",
   "metadata": {},
   "source": [
    "# Fine-tune an Existing Model\n",
    "This notebook demonstrates how to train (or fine-tune) an existing model by loading pre-trained weights and continuing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mvtech\n",
    "import torchvision.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import pytorch_ssim\n",
    "import mdn1\n",
    "from VT_AE import VT_AE as ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36529980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "config = {\n",
    "    'product': 'hazelnut',\n",
    "    'epochs': 400,\n",
    "    'learning_rate': 0.0001,\n",
    "    'patch_size': 64,\n",
    "    'batch_size': 2\n",
    "}\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c274040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensorboard writer and training variables\n",
    "writer = SummaryWriter()\n",
    "prdt = config['product']\n",
    "epoch = config['epochs']\n",
    "minloss = 1e10\n",
    "ep = 0\n",
    "ssim_loss = pytorch_ssim.SSIM()\n",
    "print(f'Training setup complete for product: {prdt}')\n",
    "print(f'Training for {epoch} epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "data = mvtech.Mvtec(config['batch_size'], product=prdt)\n",
    "print(f'Dataset loaded for {prdt}')\n",
    "print(f'Batch size: {config['batch_size']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model declaration\n",
    "model = ae(patch_size=config['patch_size'], train=True).cuda()\n",
    "G_estimate = mdn1.MDN().cuda()\n",
    "model.train()\n",
    "G_estimate.train()\n",
    "print('Models initialized and moved to GPU')\n",
    "print(f'Patch size: {config['patch_size']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing model weights if available\n",
    "model_path = f'./saved_model/VT_AE_Mvtech_{prdt}.pt'\n",
    "g_estimate_path = f'./saved_model/G_estimate_Mvtech_{prdt}.pt'\n",
    "if os.path.exists(model_path) and os.path.exists(g_estimate_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    G_estimate.load_state_dict(torch.load(g_estimate_path))\n",
    "    print('Loaded existing model weights.')\n",
    "else:\n",
    "    print('No saved weights found, training from scratch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architectures\n",
    "print('--- Architecture of VT_AE ---')\n",
    "print(model)\n",
    "print('--- Architecture of MDN ---')\n",
    "print(G_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db78c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer Declaration\n",
    "Optimiser = Adam(list(model.parameters()) + list(G_estimate.parameters()), lr=config['learning_rate'], weight_decay=0.0001)\n",
    "print(f'Optimizer initialized with learning rate: {config['learning_rate']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print('Network training started.....')\n",
    "for i in range(epoch):\n",
    "    t_loss = []\n",
    "    for j, m in data.train_loader:\n",
    "        if j.size(1) == 1:\n",
    "            j = torch.stack([j, j, j]).squeeze(2).permute(1, 0, 2, 3)\n",
    "        model.zero_grad()\n",
    "        vector, reconstructions = model(j.cuda())\n",
    "        pi, mu, sigma = G_estimate(vector)\n",
    "        loss1 = F.mse_loss(reconstructions, j.cuda(), reduction='mean')\n",
    "        loss2 = -ssim_loss(j.cuda(), reconstructions)\n",
    "        loss3 = mdn1.mdn_loss_function(vector, mu, sigma, pi)\n",
    "        print(f'loss3: {loss3.item()}')\n",
    "        loss = 5 * loss1 + 0.5 * loss2 + loss3\n",
    "        t_loss.append(loss.item())\n",
    "        writer.add_scalar('recon-loss', loss1.item(), i)\n",
    "        writer.add_scalar('ssim loss', loss2.item(), i)\n",
    "        writer.add_scalar('Gaussian loss', loss3.item(), i)\n",
    "        writer.add_histogram('Vectors', vector)\n",
    "        # writer.add_histogram('Pi', pi)\n",
    "        # writer.add_histogram('Variance', sigma)\n",
    "        # writer.add_histogram('Mean', mu)\n",
    "        loss.backward()\n",
    "        Optimiser.step()\n",
    "    writer.add_image('Reconstructed Image', utils.make_grid(reconstructions), i, dataformats='CHW')\n",
    "    writer.add_scalar('Mean Epoch loss', np.mean(t_loss), i)\n",
    "    print(f'Mean Epoch {i} loss: {np.mean(t_loss)}')\n",
    "    print(f'Min loss epoch: {ep} with min loss: {minloss}')\n",
    "    if np.mean(t_loss) <= minloss:\n",
    "        minloss = np.mean(t_loss)\n",
    "        ep = i\n",
    "        os.makedirs('./saved_model', exist_ok=True)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        torch.save(G_estimate.state_dict(), g_estimate_path)\n",
    "        print(f'New best model saved at epoch {i} with loss {minloss}')\n",
    "writer.close()\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49159b9f-c810-496f-9651-232c87f41ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mvtech\n",
    "import torchvision.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import pytorch_ssim\n",
    "import mdn1\n",
    "from VT_AE import VT_AE as ae\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0f62f51-8192-4825-9cfc-bc04c6fbcb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "product: bottle\n",
      "epochs: 400\n",
      "learning_rate: 0.0001\n",
      "patch_size: 64\n",
      "batch_size: 2\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "# Configuration parameters (modify these as needed)\n",
    "config = {\n",
    "    'product': 'bottle',  # product from the dataset MvTec or BTAD\n",
    "    'epochs': 400,          # Number of epochs to train\n",
    "    'learning_rate': 0.0001, # learning rate\n",
    "    'patch_size': 64,       # Patch size of the images\n",
    "    'batch_size': 2         # batch size\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6737287-2c7c-4afa-a8bf-f112beec0396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ae(train=False).cuda()\n",
    "G_estimate = mdn1.MDN().cuda()\n",
    "\n",
    "# Loading weights\n",
    "model.load_state_dict(torch.load(f'./saved_model/VT_AE_Mvtech_hazelnut'+'.pt'))\n",
    "G_estimate.load_state_dict(torch.load(f'./saved_model/G_estimate_Mvtech_hazelnut'+'.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a5cdc-c7b7-4fd6-9952-9226d4cf9db1",
   "metadata": {},
   "source": [
    "lemon trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "014bdb81-66ec-437e-bc66-be7e06f7e928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Architecture of whatever model this is ---\n",
      "\n",
      "VT_AE(\n",
      "  (vt): ViT(\n",
      "    (patch_to_embedding): Linear(in_features=12288, out_features=512, bias=True)\n",
      "    (transformer): Transformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x ModuleList(\n",
      "          (0): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (to_cls_token): Identity()\n",
      "  )\n",
      "  (decoder): decoder2(\n",
      "    (decoder2): Sequential(\n",
      "      (0): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): ConvTranspose2d(16, 32, kernel_size=(9, 9), stride=(3, 3), padding=(1, 1))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): ConvTranspose2d(32, 32, kernel_size=(7, 7), stride=(5, 5), padding=(1, 1))\n",
      "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): ConvTranspose2d(32, 16, kernel_size=(9, 9), stride=(2, 2))\n",
      "      (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): ConvTranspose2d(16, 8, kernel_size=(6, 6), stride=(1, 1))\n",
      "      (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): ConvTranspose2d(8, 3, kernel_size=(11, 11), stride=(1, 1))\n",
      "      (16): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (Digcap): DigitCaps()\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Architecture of mixxed density network ---\n",
      "\n",
      "MDN(\n",
      "  (pi): Linear(in_features=512, out_features=10, bias=False)\n",
      "  (mu): Linear(in_features=512, out_features=5120, bias=False)\n",
      "  (sigma_sq): Linear(in_features=512, out_features=5120, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Architecture of whatever model this is ---\\n\")\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\\n--- Architecture of mixxed density network ---\\n\")\n",
    "print(G_estimate)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9b049d8-5834-45bc-b8ea-53f3cfed0c0b",
   "metadata": {},
   "source": [
    "load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5dba98e-a58a-4cfa-b8f6-e4a15d8f47f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train images of good bottle are: 209\n",
      "total test images of broken_large bottle are: 20\n",
      "total test images of broken_small bottle are: 22\n",
      "total test images of contamination bottle are: 21\n",
      "the good images for test images of good bottle is not included in the test anomolous data\n",
      "total ground_truth images of broken_large bottle are: 20\n",
      "total ground_truth images of broken_small bottle are: 22\n",
      "total ground_truth images of contamination bottle are: 21\n",
      "total test images of good bottle are: 20\n",
      " --Size of bottle train loader: torch.Size([209, 3, 512, 512])--\n",
      " --Size of bottle test anomaly loader: torch.Size([63, 3, 512, 512])--\n",
      " --Size of bottle test normal loader: torch.Size([20, 3, 512, 512])--\n",
      " --Total Image in bottle Validation loader: 20--\n",
      "Dataset loaded for bottle\n",
      "Batch size: 2\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data = mvtech.Mvtec(config[\"batch_size\"], product=prdt)\n",
    "\n",
    "print(f\"Dataset loaded for {config['product']}\")\n",
    "print(f\"Batch size: {config['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68ba71fb-1970-40a1-910e-c2cdd2b0f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized with learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Optimizer Declaration\n",
    "Optimiser = Adam(\n",
    "    list(model.parameters()) + list(G_estimate.parameters()), \n",
    "    lr=config[\"learning_rate\"], \n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "print(f\"Optimizer initialized with learning rate: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff9a1701-2550-47d0-bea6-6a008c11ec48",
   "metadata": {},
   "source": [
    "freeze everything except for everything exculding the vit layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb1d6138-19a9-487a-9bb0-96f0143dfdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_vit_except_lora(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'lora_A') and module.lora_A is not None:\n",
    "            module.lora_A.requires_grad = True\n",
    "        if hasattr(module, 'lora_B') and module.lora_B is not None:\n",
    "            module.lora_B.requires_grad = True\n",
    "\n",
    "freeze_vit_except_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "155af580-b976-4742-9cc8-d39e73eb4f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network training started.....\n",
      "loss3: 1377765.875\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "loss3: nan\n",
      "Mean Epoch 0 loss: nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Epoch loss\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(t_loss), i)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(t_loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin loss epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with min loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(t_loss) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m minloss:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ep' is not defined"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below if you want to track errors\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "print('\\nNetwork training started.....')\n",
    "\n",
    "for i in range(config['epochs']):\n",
    "    t_loss = []\n",
    "    \n",
    "    for j, m in data.train_loader:\n",
    "        # Handle grayscale images by converting to 3-channel\n",
    "        if j.size(1) == 1:\n",
    "            j = torch.stack([j, j, j]).squeeze(2).permute(1, 0, 2, 3)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        vector, reconstructions = model(j.cuda())\n",
    "        pi, mu, sigma = G_estimate(vector)\n",
    "        \n",
    "        # Loss calculations\n",
    "        loss1 = F.mse_loss(reconstructions, j.cuda(), reduction='mean')  # Reconstruction Loss\n",
    "        loss2 = -ssim_loss(j.cuda(), reconstructions)  # SSIM loss for structural similarity\n",
    "        loss3 = mdn1.mdn_loss_function(vector, mu, sigma, pi)  # MDN loss for gaussian approximation\n",
    "        \n",
    "        print(f'loss3: {loss3.item()}')\n",
    "        loss = 5 * loss1 + 0.5 * loss2 + loss3  # Total loss\n",
    "        \n",
    "        t_loss.append(loss.item())  # storing all batch losses to calculate mean epoch loss\n",
    "        \n",
    "        # Tensorboard logging\n",
    "        writer.add_scalar('recon-loss', loss1.item(), i)\n",
    "        writer.add_scalar('ssim loss', loss2.item(), i)\n",
    "        writer.add_scalar('Gaussian loss', loss3.item(), i)\n",
    "        writer.add_histogram('Vectors', vector)\n",
    "        \n",
    "        ## Uncomment below to store the distributions of pi, var and mean ##        \n",
    "        # writer.add_histogram('Pi', pi)\n",
    "        # writer.add_histogram('Variance', sigma)\n",
    "        # writer.add_histogram('Mean', mu)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        Optimiser.step()\n",
    "    \n",
    "    # Log epoch-level information\n",
    "    writer.add_image('Reconstructed Image', utils.make_grid(reconstructions), i, dataformats='CHW')\n",
    "    writer.add_scalar('Mean Epoch loss', np.mean(t_loss), i)\n",
    "    \n",
    "    print(f'Mean Epoch {i} loss: {np.mean(t_loss)}')\n",
    "    print(f'Min loss epoch: {ep} with min loss: {minloss}')\n",
    "    \n",
    "    # Save the best model\n",
    "    if np.mean(t_loss) <= minloss:\n",
    "        minloss = np.mean(t_loss)\n",
    "        ep = i\n",
    "        os.makedirs('./saved_model', exist_ok=True)\n",
    "        torch.save(model.state_dict(), f'./saved_model/VT_AE_Mvtech_{prdt}_peft.pt')\n",
    "        torch.save(G_estimate.state_dict(), f'./saved_model/G_estimate_Mvtech_{prdt}_peft.pt')\n",
    "        print(f\"New best model saved at epoch {i} with loss {minloss}\")\n",
    "\n",
    "writer.close()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6f45c-a761-4593-9f90-c5fbcebda774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "939306f6",
   "metadata": {},
   "source": [
    "# Fine-tune Existing Model\n",
    "This notebook demonstrates how to train (or continue training) an existing model by loading its weights and running the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d619721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mvtech\n",
    "import torchvision.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import pytorch_ssim\n",
    "import mdn1\n",
    "from VT_AE import VT_AE as ae\n",
    "import torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VT)",
   "language": "python",
   "name": "vt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
